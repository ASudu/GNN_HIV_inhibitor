{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "\n",
    "The dataset contains **40,000 molecules** and labelled ``0`` or ``1`` based on their ability to inhibit **HIV**\n",
    "\n",
    "We split our project into 5 modules:\n",
    "\n",
    "1) Buiding the **dataset** - converting raw data into useful node and edge features\n",
    "2) Building the **GNN**\n",
    "3) **Generative GNN** - to generate arbitrary molecules that are potential HIV inhibitors\n",
    "4) **Explainable AI** on graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Buiding the dataset - converting raw data into useful node and edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "\n",
    "path = \"data/raw/HIV.csv\"\n",
    "dataset = pd.read_csv(path)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first column is the actual set of molecules. We want to convert these to graph so that we can pass as data to GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General info about dataset\n",
    "print(dataset.shape)\n",
    "print(dataset[\"HIV_active\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **41127 molecules** and out of that only **1443 molecules** are actually HIV inhibitors. We keep this in mind when selecting training and testing data. We might have to balance the dataset by under-sampling the negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the smiles column to molecule structure\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "# See for a small subset of molecules\n",
    "sample_smiles = dataset[\"smiles\"][4:30].values\n",
    "sample_mols = [Chem.MolFromSmiles(smiles) for smiles in sample_smiles]\n",
    "grid = Draw.MolsToGridImage(sample_mols,molsPerRow=8,subImgSize=(200,200))\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen a few of the molecules as graph, let us move to creating the custom dataset using PyTorch Geometric. We use the method mentioned in Documentation that corresponds to creating a dataset that stores in your local machine instead of RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# We extend the functionalites in Dataset class\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, root, filename, test=False, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = Where the dataset should be stored. This folder is split\n",
    "        into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
    "        \"\"\"\n",
    "        self.test = test\n",
    "        self.filename = filename\n",
    "        super(MoleculeDataset, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
    "            (The download func. is not implemented here)  \n",
    "        \"\"\"\n",
    "        return self.filename\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "\n",
    "        if self.test:\n",
    "            return [f'data_test_{i}.pt' for i in list(self.data.index)]\n",
    "        else:\n",
    "            return [f'data_{i}.pt' for i in list(self.data.index)]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"In this function we construct the graphs for each object in dataset\n",
    "        \"\"\"\n",
    "        # Reading the dataset\n",
    "        self.data = pd.read_csv(self.raw_paths[0])\n",
    "\n",
    "        # Iterating over each object to get the required parameters to preprocess\n",
    "        # tqdm gives process bar which tells us how far our process is done\n",
    "        for index, mol in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
    "            mol_obj = Chem.MolFromSmiles(mol[\"smiles\"])\n",
    "            # Get node features\n",
    "            node_feats = self._get_node_features(mol_obj)\n",
    "            # Get edge features\n",
    "            edge_feats = self._get_edge_features(mol_obj)\n",
    "            # Get adjacency info\n",
    "            edge_index = self._get_adjacency_info(mol_obj)\n",
    "            # Get labels info\n",
    "            label = self._get_labels(mol[\"HIV_active\"])\n",
    "\n",
    "            # Create data object\n",
    "            data = Data(x=node_feats, \n",
    "                        edge_index=edge_index,\n",
    "                        edge_attr=edge_feats,\n",
    "                        y=label,\n",
    "                        smiles=mol[\"smiles\"]\n",
    "                        ) \n",
    "            if self.test:\n",
    "                torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{index}.pt'))\n",
    "            else:\n",
    "                torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_{index}.pt'))\n",
    "\n",
    "    # Based on domain knowledge we decide to get the node features\n",
    "    def _get_node_features(self, mol):\n",
    "        \"\"\" \n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        all_node_feats = []\n",
    "\n",
    "        # Iterating over each ATOM of the passed molecule object\n",
    "        for atom in mol.GetAtoms():\n",
    "            node_feats = []\n",
    "\n",
    "            # Feature 1: Atomic number        \n",
    "            node_feats.append(atom.GetAtomicNum())\n",
    "            # Feature 2: Atom degree\n",
    "            node_feats.append(atom.GetDegree())\n",
    "            # Feature 3: Formal charge\n",
    "            node_feats.append(atom.GetFormalCharge())\n",
    "            # Feature 4: Hybridization\n",
    "            node_feats.append(atom.GetHybridization())\n",
    "            # Feature 5: Aromaticity\n",
    "            node_feats.append(atom.GetIsAromatic())\n",
    "            # Feature 6: Total Num Hs\n",
    "            node_feats.append(atom.GetTotalNumHs())\n",
    "            # Feature 7: Radical Electrons\n",
    "            node_feats.append(atom.GetNumRadicalElectrons())\n",
    "            # Feature 8: In Ring\n",
    "            node_feats.append(atom.IsInRing())\n",
    "            # Feature 9: Chirality\n",
    "            node_feats.append(atom.GetChiralTag())\n",
    "\n",
    "            # Append node features to matrix\n",
    "            all_node_feats.append(node_feats)\n",
    "\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "    # Based on domain knowledge we decide to get the edge features\n",
    "    def _get_edge_features(self, mol):\n",
    "        \"\"\" \n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of edges, Edge Feature size]\n",
    "        \"\"\"\n",
    "        all_edge_feats = []\n",
    "\n",
    "        # Iterating over each BOND of the passed molecule object\n",
    "        for bond in mol.GetBonds():\n",
    "            edge_feats = []\n",
    "\n",
    "            # Feature 1: Bond type (as double)\n",
    "            edge_feats.append(bond.GetBondTypeAsDouble())\n",
    "            # Feature 2: Rings\n",
    "            edge_feats.append(bond.IsInRing())\n",
    "\n",
    "            # Append node features to matrix (twice, per direction)\n",
    "            all_edge_feats += [edge_feats, edge_feats]\n",
    "\n",
    "        all_edge_feats = np.asarray(all_edge_feats)\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_adjacency_info(self, mol):\n",
    "        \"\"\"\n",
    "        We could also use rdmolops.GetAdjacencyMatrix(mol)\n",
    "        but we want to be sure that the order of the indices\n",
    "        matches the order of the edge features\n",
    "        \"\"\"\n",
    "        edge_indices = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_indices += [[i, j], [j, i]]\n",
    "\n",
    "        edge_indices = torch.tensor(edge_indices)\n",
    "        edge_indices = edge_indices.t().to(torch.long).view(2, -1)\n",
    "        return edge_indices\n",
    "\n",
    "    def _get_labels(self, label):\n",
    "        label = np.asarray([label])\n",
    "        return torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
    "            - Is not needed for PyG's InMemoryDataset\n",
    "        \"\"\"\n",
    "        if self.test:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{idx}.pt'))\n",
    "        else:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_{idx}.pt'))   \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset\n",
    "dataset = MoleculeDataset(root=\"data/\",filename='HIV.csv')\n",
    "\n",
    "print(dataset[0].edge_index.t())\n",
    "print(dataset[0].x)\n",
    "print(dataset[0].y)\n",
    "print(dataset[0].edge_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the pre-processing is done, we can move on to building the GNN model to do graph-level predictions.\n",
    "\n",
    "### Step 2: Building the GNN model\n",
    "\n",
    "#### Task 1: Graph classification\n",
    "\n",
    "Given a molecule, we classify it whether it is a HIV inhibitor (``1``) or not (``0``). For this task we need a feture vector for the whole graph instead of node-features or edge-features. There are a lot of approaches for this. We discuss a couple of them below:\n",
    "\n",
    "1) **Naive pooling** - Apply mean/max/sum pooling to all node features to get the graph representation\n",
    "2) **Hierarchical pooling** - similar to pooling done in images; at each step we share feature with neighbors. So, at each step we can drop the nodes in a specific manner so that finally we are left with a single node whose feature gives us the feature representation of the graph. The choice of nodes to drop is either done by either **Differential pooling** , which clusters nodes and pools its feature vectors and new graph with clusters as nodes is formed, or **Top-K pooling**, which squeezes the feature vectors to a single vector and top-K nodes are considered.\n",
    "3) **Super/virtual/dummy node** - All nodes will pass their vectors to the super node during message passing but this node won't share its vector to others. The vector obtained by this node is the graph representation\n",
    "\n",
    "First we do the DATASET changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset changes\n",
    "# Instead of manually assigning features here we us FEATURIZER by DeepChem\n",
    "\n",
    "# import deepchem as dc\n",
    "\n",
    "# class MoleculeDataset(Dataset):\n",
    "#     def __init__(self, root, filename, test=False, transform=None, pre_transform=None):\n",
    "#         \"\"\"\n",
    "#         root = Where the dataset should be stored. This folder is split\n",
    "#         into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
    "#         \"\"\"\n",
    "#         self.test = test\n",
    "#         self.filename = filename\n",
    "#         super(MoleculeDataset, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "#     @property\n",
    "#     def raw_file_names(self):\n",
    "#         \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
    "#             (The download func. is not implemented here)  \n",
    "#         \"\"\"\n",
    "#         return self.filename\n",
    "\n",
    "#     @property\n",
    "#     def processed_file_names(self):\n",
    "#         \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
    "#         self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "\n",
    "#         if self.test:\n",
    "#             return [f'data_test_{i}.pt' for i in list(self.data.index)]\n",
    "#         else:\n",
    "#             return [f'data_{i}.pt' for i in list(self.data.index)]\n",
    "        \n",
    "\n",
    "#     def download(self):\n",
    "#         pass\n",
    "\n",
    "#     # Important changes compared to previous version of the class definition\n",
    "#     def process(self):\n",
    "#         # Reading the dataset\n",
    "#         self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "\n",
    "#         # KEY STEP - using Featurizer\n",
    "#         featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "\n",
    "#         # Iterating over each object to get the required parameters to preprocess\n",
    "#         # tqdm gives process bar which tells us how far our process is done\n",
    "#         for index, row in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
    "#             # Featurize molecule\n",
    "#             mol = Chem.MolFromSmiles(row[\"smiles\"])\n",
    "#             f = featurizer._featurize(mol)\n",
    "#             data = f.to_pyg_graph()\n",
    "#             data.y = self._get_label(row[\"HIV_active\"])\n",
    "#             data.smiles = row[\"smiles\"]\n",
    "\n",
    "#             # Naming the processed data files\n",
    "#             if self.test:\n",
    "#                 torch.save(data, \n",
    "#                     os.path.join(self.processed_dir, \n",
    "#                                  f'data_test_{index}.pt'))\n",
    "#             else:\n",
    "#                 torch.save(data, \n",
    "#                     os.path.join(self.processed_dir, \n",
    "#                                  f'data_{index}.pt'))\n",
    "            \n",
    "\n",
    "#     def _get_label(self, label):\n",
    "#         label = np.asarray([label])\n",
    "#         return torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "#     def len(self):\n",
    "#         return self.data.shape[0]\n",
    "\n",
    "#     def get(self, idx):\n",
    "#         \"\"\" - Equivalent to __getitem__ in pytorch\n",
    "#             - Is not needed for PyG's InMemoryDataset\n",
    "#         \"\"\"\n",
    "#         if self.test:\n",
    "#             data = torch.load(os.path.join(self.processed_dir, \n",
    "#                                  f'data_test_{idx}.pt'))\n",
    "#         else:\n",
    "#             data = torch.load(os.path.join(self.processed_dir, \n",
    "#                                  f'data_{idx}.pt'))        \n",
    "#         return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling to balance the dataset\n",
    "# Load raw dataset\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/raw/HIV_train.csv\")\n",
    "data.index = data[\"index\"]\n",
    "data[\"HIV_active\"].value_counts()\n",
    "start_index = data.iloc[0][\"index\"]\n",
    "\n",
    "# Apply oversampling\n",
    "\n",
    "# Check how many additional samples we need\n",
    "neg_class = data[\"HIV_active\"].value_counts()[0]\n",
    "pos_class = data[\"HIV_active\"].value_counts()[1]\n",
    "multiplier = int(neg_class/pos_class) - 1\n",
    "\n",
    "# Replicate the dataset for the positive class\n",
    "replicated_pos = [data[data[\"HIV_active\"] == 1]]*multiplier\n",
    "\n",
    "# Append replicated data\n",
    "data = pd.concat([data, pd.DataFrame([replicated_pos])], axis=0)\n",
    "print(data.shape)\n",
    "\n",
    "# Shuffle dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Re-assign index (This is our ID later)\n",
    "index = range(start_index, start_index + data.shape[0])\n",
    "data.index = index\n",
    "data[\"index\"] = data.index\n",
    "data.head()\n",
    "\n",
    "# %% Save\n",
    "data.to_csv(\"data/raw/HIV_train_oversampled.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already done these manually instead of using Deepchem so we comment it out for now. Next, we build the GNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "from torch_geometric.nn import TransformerConv, GATConv, TopKPooling, BatchNorm\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.nn.conv.x_conv import XConv\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, feature_size):\n",
    "        super(GNN,self).__init__()\n",
    "        num_classes = 2\n",
    "        embedding_size = 1024\n",
    "\n",
    "\n",
    "        # GNN layers\n",
    "        # Since the molecule classes are not too big, we use 3 layers\n",
    "        # 3 attention heads produce 3 different output vectors so we use linear layer to get a simple embedding\n",
    "        self.conv1 = GATConv(feature_size, embedding_size, heads=3, dropout=0.3)\n",
    "        self.head_transform1 = Linear(embedding_size*3,embedding_size)\n",
    "        self.pool1 = TopKPooling(embedding_size,ratio=0.8)\n",
    "        self.conv2 = GATConv(embedding_size, embedding_size, heads=3, dropout=0.3)\n",
    "        self.head_transform2 = Linear(embedding_size*3,embedding_size)\n",
    "        self.pool2 = TopKPooling(embedding_size,ratio=0.5)\n",
    "        self.conv3 = GATConv(embedding_size, embedding_size, heads=3, dropout=0.3)\n",
    "        self.head_transform3 = Linear(embedding_size*3,embedding_size)\n",
    "        self.pool3 = TopKPooling(embedding_size,ratio=0.2)\n",
    "\n",
    "        # Linear layers\n",
    "        self.linear1 = Linear(embedding_size*2,1024)\n",
    "        self.linear2 = Linear(1024,num_classes)\n",
    "    \n",
    "    def forward(self,x,edge_attr,edge_index,batch_index):\n",
    "        \"\"\"Does the forward pass of the feature vectors through each of the blocks defined above\n",
    "        \"\"\"\n",
    "\n",
    "        # First block\n",
    "        x = self.conv1(x,edge_index)\n",
    "        x = self.head_transform1(x)         # converting back to embedding shape\n",
    "\n",
    "        # Forming the new graph\n",
    "        x, edge_index, edge_attr, batch_index, _, _ = self.pool1(x,edge_index,None,batch_index)\n",
    "\n",
    "        # First output vector (by the first attention head)\n",
    "        x1 = torch.cat([gmp(x,batch_index),gap(x,batch_index)],dim=1)\n",
    "\n",
    "\n",
    "        # Second block\n",
    "        x = self.conv2(x,edge_index)\n",
    "        x = self.head_transform2(x)         # converting back to embedding shape\n",
    "\n",
    "        # Forming the new graph\n",
    "        x, edge_index, edge_attr, batch_index, _, _ = self.pool2(x,edge_index,None,batch_index)\n",
    "\n",
    "        # Second output vector (by the second attention head)\n",
    "        x2 = torch.cat([gmp(x,batch_index),gap(x,batch_index)],dim=1)\n",
    "\n",
    "\n",
    "        # Third block\n",
    "        x = self.conv3(x,edge_index)\n",
    "        x = self.head_transform3(x)         # converting back to embedding shape\n",
    "\n",
    "        # Forming the new graph\n",
    "        x, edge_index, edge_attr, batch_index, _, _ = self.pool3(x,edge_index,None,batch_index)\n",
    "\n",
    "        # Third output vector (by the third attention head)\n",
    "        x3 = torch.cat([gmp(x,batch_index),gap(x,batch_index)],dim=1)\n",
    "\n",
    "\n",
    "        # Concat pooled vectors\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "\n",
    "        # Output block\n",
    "        x = self.linear1(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built the model, now we move onto to TRAINING and OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# imports\n",
    "from torch_geometric.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "import mlflow.pytorch\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Loading dataset\n",
    "train_dataset = MoleculeDataset(root=\"data/\",filename=\"HIV_train_oversampled.csv\")\n",
    "test_dataset = MoleculeDataset(root=\"data/\",filename=\"HIV_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading GNN model\n",
    "model = GNN(feature_size=train_dataset[0].x.shape[1])\n",
    "model = model.to(device)\n",
    "print(f\"Number of parameter: {count_parameters(model)}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "weights = torch.tensor([1,10], dtype=torch.float32).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)       # Exponentially decay lr at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training\n",
    "NUM_GRAPHS_PER_BATCH = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(epoch):\n",
    "    # Enumerate over data\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for _, batch in enumerate(tqdm(train_loader)):\n",
    "        batch.to(device)\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Passing node features and connection info\n",
    "        pred = model(batch.x.float(), batch.edge_attr.float(), batch.edge_index, batch.batch)\n",
    "        # Calculating loss and gradients\n",
    "        loss = torch.sqrt(loss_fn(pred,batch.y))\n",
    "        loss.backward()\n",
    "        # Update using gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        all_preds.append(np.argmax(pred.cpu().detach().numpy(), axis=1))\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    calculate_metrics(all_preds, all_labels, epoch, \"train\")\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def test(epoch):\n",
    "    # Enumerate over data\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch in test_loader:\n",
    "        batch.to(device)\n",
    "        # Passing node features and connection info\n",
    "        pred = model(batch.x.float(), batch.edge_attr.float(), batch.edge_index, batch.batch)\n",
    "        # Calculating loss and gradients\n",
    "        loss = torch.sqrt(loss_fn(pred,batch.y))\n",
    "        all_preds.append(np.argmax(pred.cpu().detach().numpy(), axis=1))\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    calculate_metrics(all_preds, all_labels, epoch, \"train\")\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def calculate_metrics(y_pred, y_true, epoch, type):\n",
    "    print(f\"\\n Confusion matrix: \\n {confusion_matrix(y_true, y_pred)}\")\n",
    "    print(f\"F1 score: {f1_score(y_true, y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred)}\")\n",
    "    print(f\"Recall: {recall_score(y_true, y_pred)}\")\n",
    "    try:\n",
    "        roc = roc_auc_score(y_true, y_pred)\n",
    "        print(f\"ROC AUC: {roc}\")\n",
    "        mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(roc), step=epoch)\n",
    "    except:\n",
    "        mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(0), step=epoch)\n",
    "        print(f\"ROC AUC: notdefined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "with mlflow.start_run() as run:\n",
    "    for epoch in range(50):\n",
    "        # set as training\n",
    "        model.train()\n",
    "        # call train fn\n",
    "        loss = train(epoch=epoch)\n",
    "        loss = loss.cpu().detach().numpy()\n",
    "        print(f\"Epoch {epoch}   | Train loss {loss}\")\n",
    "        mlflow.log_metric(key=\"Train loss\", value=float(loss), step=epoch)\n",
    "\n",
    "        # set as testing\n",
    "        model.eval()\n",
    "        if epoch % 5 == 0:\n",
    "            # call test fn\n",
    "            loss = test(epoch=epoch)\n",
    "            print(f\"Epoch {epoch}   | Test loss {loss}\")\n",
    "            mlflow.log_metric(key=\"Test loss\", value=float(loss), step=epoch)\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "mlflow.pytorch.log_model(model, \"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
